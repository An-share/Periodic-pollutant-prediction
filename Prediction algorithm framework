import pandas as pd
from pandas import read_csv
from datetime import datetime
from matplotlib import pyplot
import matplotlib.pyplot as plt
from sklearn.preprocessing import LabelEncoder,MinMaxScaler
from sklearn.metrics import mean_squared_error
from sklearn.metrics import mean_absolute_error
from sklearn.metrics import r2_score
from tensorflow.keras.models import Sequential
from keras.layers import Dense,Dropout
from keras.layers import LSTM
from numpy import concatenate
from math import sqrt
from pylab import mpl
# Read the data
df = pd.read_csv('file.csv',  parse_dates={'dt' : ['year', 'month', 'day','hour']}, #sep=";", infer_datetime_format=True, low_memory=False, na_values=['nan','?'], index_col='dt')
df.drop('No', axis=1, inplace=True)
print(df.head(5))
dataset_train = dataset_train.reset_index()
cols = list(dataset_train.columns[1:])# Exclude the 'Timestamp' column
datelist_train = list(dataset_train['dt'])
datelist_train = [date for date in datelist_train]
training_set = dataset_train[cols].values 
from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
training_set_scaled = sc.fit_transform(training_set)
sc_predict = StandardScaler()
sc_predict.fit_transform(training_set[:, 0:1])
X_train = []
y_train = []
n_future =replace 
n_past = repalce
for i in range(n_past, len(training_set_scaled) - n_future +1):
    X_train.append(training_set_scaled[i - n_past:i,0:dataset_train.shape[1]])
    y_train.append(training_set_scaled[i+n_future-1:i+n_future, 2])
X_train, y_train = np.array(X_train), np.array(y_train)
X_train, X_test, y_train, y_test = train_test_split(X_train, y_train, test_size=0.2, random_state=42)
def lr_schedule(epoch):
    initial_learning_rate = 1e-3
    decay_rate = 0.5
    decay_steps = 10
    if epoch < decay_steps:
        return initial_learning_rate
    else:
        return initial_learning_rate * decay_rate ** ((epoch - decay_steps) // decay_steps)
lr_scheduler = tf.keras.callbacks.LearningRateScheduler(lr_schedule)
input_shape = X_train.shape[1:]
inputs = keras.Input(shape=input_shape)
#Specific content replacement of different models
class SpatialAttention(tf.keras.layers.Layer):
    def __init__(self):
        super(SpatialAttention, self).__init__()
    def build(self, input_shape):
        self.conv = Conv2D(1, kernel_size=3, activation='relu', padding='same')
        self.pooling = GlobalAveragePooling2D()
    def call(self, inputs):
        avg_pool = self.pooling(inputs)
        avg_pool = tf.expand_dims(avg_pool, axis=-1)
        attention = self.conv(inputs)
        return inputs * attention
def residual_block(input_layer):   
    input_layer_adjusted = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(input_layer)
    output_layer_adjusted = Conv2D(filters=64, kernel_size=(3, 3), padding='same', activation='relu')(input_layer_adjusted)
    residual = Add()([input_layer, output_layer_adjusted])
    return residual
conv_layer = Conv2D(filters=64, kernel_size=(3, 3), strides=1, padding="same", activation='relu')(inputs)
pool_layer = layers.MaxPool2D(pool_size=6, strides=1, padding="same")(conv_layer)
conv_layer_2 = Conv2D(64, kernel_size=(3, 3), strides=1, padding="same", activation='relu')(pool_layer)
attention_layer = SpatialAttention()(conv_layer_2)
reshaped_conv_layer = Reshape((-1, attention_layer.shape[-1] * attention_layer.shape[-2]))(attention_layer)
lstm_layer1 = Bidirectional(LSTM(64,  activation='sigmoid',return_sequences=True))(reshaped_conv_layer)
reshaped_lstm_layer1 = Reshape()(lstm_layer1)
residual_1 = residual_block(reshaped_lstm_layer1)
reshaped_residual_1 = Reshape()(residual_1)
temporal_attention = tf.keras.layers.Attention()([reshaped_residual_1 ,reshaped_residual_1 ])
temporal_attention = Reshape())(temporal_attention)
combined_output = tf.keras.layers.Add()([residual_1 , temporal_attention])
dropout_layer = Dropout(0.1)(combined_output)
reshaped_dropout_layer = Reshape()(dropout_layer)
lstm_layer2 = Bidirectional(LSTM(64, activation='sigmoid',return_sequences=False))(reshaped_dropout_layer )

def correlation_coefficient(y_true, y_pred):    
    mean_pred = tf.reduce_mean(y_pred)
    mean_true = tf.reduce_mean(y_true)
    xm = y_pred - mean_pred
    ym = y_true - mean_true
    r_num = tf.reduce_sum(tf.multiply(xm, ym))
    r_den = tf.sqrt(tf.reduce_sum(tf.square(xm)) * tf.reduce_sum(tf.square(ym)))
    r = r_num / r_den
    return r
def total_inertia_criterion(y_true, y_pred):    
    tss = tf.reduce_sum(tf.square(y_true - tf.reduce_mean(y_true)))
    rss = tf.reduce_sum(tf.square(y_true - y_pred))
    tic = 1 - (rss / tss)
    return tic
def index_of_agreement(y_true, y_pred):    
    mean_true = tf.reduce_mean(y_true)
    ia_numerator = tf.reduce_sum(tf.square(tf.subtract(y_true, y_pred)))
    ia_denominator = tf.reduce_sum(tf.square(tf.abs(tf.subtract(y_true, mean_true)))) + tf.reduce_sum(tf.square(tf.abs(tf.subtract(y_pred, mean_true))))
    ia = 1 - (ia_numerator / ia_denominator)
    return ia
model = tf.keras.Model(inputs=inputs, outputs=output)
model.compile(optimizer='adam', loss='mse', metrics=["msle", "mae", "mape", "RootMeanSquaredError", correlation_coefficient, total_inertia_criterion, index_of_agreement])
model.summary()
history = model.fit(X_train, y_train, epochs=150, validation_data=(X_test, y_test), callbacks=[lr_scheduler])
history_dict = history.history
history_df = pd.DataFrame(history_dict)
history_df.to_csv(results.csv', index=False)
